,Titulo,Autores,Enlaces,Resumen
1,"
When Are Learning Analytics Ready and What Are They Ready For
","
                                                                                    Alyssa F Wise,                                                            Simon Knight,                                                            Xavier Ochoa                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6360,"Learning Analytics as a field of inquiry and community is distinct in the way that it brings together in shared pursuit, the research and practice of a particular kind of educational technology. At times this relationship approaches symbiosis: the annual LAK conference offers opportunities to learn both about the latest theoretical, methodological, and technological innovations as well as challenges and effective strategies for using such innovations to support learning in real world contexts. At other times, we feel pulled in multiple directions by the different priorities of each endeavor. Research is first and foremost concerned with advancing the state of the field by building knowledge, theories, techniques and tools with generalizable implications. Practice is primarily focused on action and implementation to have a positive impact on real world learning contexts. While these aims are not unrelated, they often offer quite different answers to the question of when a learning analytics application is ready to be used in an authentic educational setting with actual learners (and real consequences). At the extreme pole of a research perspective, there is always the temptation to try one more way to optimize an analytic (have we considered all possible features, tried all appropriate algorithms, tweaked all available hyper-parameters, explored all possible visualizations etc.). But from the perspective of practice with pressing problems to address, a tool that is available and sufficiently optimised is better than an unavailable perfect one. The ultimate goal is increased impact on learning not simply improved model accuracy. Of course, as a scholarly pursuit, the field of learning analytics does not seek to only develop and implement innovative data-based technologies, but also develop a knowledge base around them. Thus the key question is how can we make a difference in the world while also engaging in a rigorous knowledge producing process?"
2,"
The SHEILA Framework: Informing Institutional Strategies and Policy Processes of Learning Analytics
","
                                                                                    Yi-Shan Tsai,                                                            Pedro Manuel Moreno-Marcos,                                                            Ioana Jivet,                                                            Maren Scheffel,                                                            Kairit Tammets,                                                            Kaire Kollom,                                                            Dragan Gašević                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6096,"This paper introduces a learning analytics policy and strategy framework developed by a cross-European research project team — SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed adapting the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents four case studies to illustrate the development process of the SHEILA framework and how it can be used iteratively to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts. To this end, the selected cases were analyzed at two stages, each a year apart, to investigate the progression of adoption approaches that were followed to solve existing challenges, and identify new challenges that could be addressed by following the SHEILA framework."
3,"
Scaling Effective Learning Strategies: Retrieval Practice and Long-Term Knowledge Retention in MOOCs
","
                                                                                    Dan Davis,                                                            René Kizilcec,                                                            Claudia Hauff,                                                            Geert-Jan Houben                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6098, 
4,"
Learn From Your (Markov) Neighbor: Coenrollment, Assortativity, and Grade Prediction in Undergraduate Courses
","
                                                                                    Joshua Patrick Gardner,                                                            Christopher Brooks,                                                            Warren Li                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6146,"In this paper, we evaluate the complete undergraduate co-enrollment network over a decade of education at a large American public university. We provide descriptive and exploratory analyses of the network, demonstrating that the co-enrollment networks evaluated follow power-law degree distributions similar to many other large-scale networks; that they reveal strong performance-based assortativity; and that network-based features can improve GPA-based student performance predictors. We model the university-wide undergraduate co-enrollment network as an undirected graph, and implement multiple network-augmented approaches to student grade prediction, including an adaption of the structural modelling approach from (Getoor, 2005,  Lu, 2003}. We compare the performance of this predictor to traditional methods used for grade prediction in undergraduate university courses, and demonstrate that a multi-view ensembling approach outperforms both prior ``flat'' and network-based models for grade prediction across several classification metrics. These findings demonstrate the usefulness of combining diverse approaches in models of student success, and demonstrate specific network-based modelling strategies that are likely to be most effective for grade prediction."
5,"
Taken Together: Conceptualizing Students’ Concurrent Course Enrollment across the Post-Secondary Curriculum using temporal analytics
","
                                                                                    Michael Brown,                                                            R. Matthew DeMonbrun,                                                            Stephanie Teasley                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6100,"In this study, we develop and test four measures for conceptualizing the potential impact of co-enrollment in different courses on students’ changing risk for academic difficulty and recovery from academic difficulty in a focal course. We offer four predictors, two related to instructional complexity and two related to structural complexity (the organization of the curriculum) that highlight different trends in student experience of the focal course. Course difficulty, discipline of major, time in semester, and simultaneous difficulty across courses were all significantly related to entering a moderate and high-risk classification in the early warning system (EWS). Course difficulty, discipline of major, and time in semester were related to exiting academic difficulty classifications. We observe a snowball effect, whereby students who are experiencing difficulty in the focal course are at increased risk of experiencing difficulty in their other courses. Our findings suggest that different metrics may be needed to identify risk for academic difficulty and recovery from academic difficulty. Our results demonstrate what a more holistic assessment of academic functioning might look like in early warning systems and course recommender systems, and suggest that academic planners consider the relationship between course co-enrollment and student academic success."
6,"
Exploratory versus Explanatory Visual Learning Analytics:  Driving Teachers’ Attention through Educational Data Storytelling
","
                                                                                    Vanessa Echeverria,                                                            Roberto Martinez-Maldonado,                                                            Simon Buckingham Shum,                                                            Katherine Chiluiza,                                                            Roger Granda,                                                            Cristina Conati                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6114,"From a human-centred computing perspective, supporting the interpretation of educational dashboards and visualizations by the people intended to use them exposes critical design challenges that may often be trivialized. Empirical evidence already shows that “usable” visualizations are not necessarily effective from an educational perspective. Since an educator’s interpretation of visualized data is essentially the construction of a narrative about student progress, in this paper, we propose the concept of “Educational Data Storytelling” as an approach for explaining student data by aligning educational visualizations with the intended learning design. We present a pilot study that explores the effectiveness of these data storytelling elements based on educator responses to prototypes by analyzing the kinds of stories they articulate, their eye-tracking behaviour, and their preferences after inspecting a series of student data visualizations. The dual purpose is to understand the contribution of each visual element for data storytelling, as well as the effectiveness of the enhancements when combined."
7,"
Gaze-Driven Design Insights to Amplify Debugging Skills: A Learner-Centered Analysis Approach
","
                                                                                    Katerina Mangaroska,                                                            Kshitij Sharma,                                                            Michail Giannakos,                                                            Hallvard Trætteberg,                                                            Pierre Dillenbourg                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6119,"This study investigates how multimodal user-generated data can be used to reinforce learner reflection, improve teaching practices, and close the learning analytics loop. In particular, the aim of the study is to utilize user gaze and action-based data to examine the role of a mirroring tool (i.e., Exercise View in Eclipse) in orchestrating basic behavioural regulation during debugging. The results demonstrated that students who processed the information presented in the Exercise View and acted upon it, improved their performance and achieved a higher level of success than those who failed to do so. The findings shed light on what constitutes relevant data within a particular learning context in programming using gaze patterns. Moreover, these findings could guide the collection of essential learner-centred analytics for designing usable, modular learning environments based on data-driven approaches."
8,"
Using Temporal Analytics to Detect Inconsistencies Between Learning Design and Students’ Behaviours
","
                                                                                    Quan Nguyen,                                                            Michal Huptych,                                                            Bart Rienties                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6141,"Extensive research in learning science has established the importance of time management in online learning. Recently, learning analytics (LA) has shed further lights on the temporal characteristics of learning by allowing researchers to capture authentic digital footprints of student learning behaviours. Nonetheless, students’ timing of engagement and its relation to learning design (LD) and academic performance have received limited attention. This study investigates to what extent students’ timing of engagement aligned with instructor learning design, and how engagement varied across different levels of performance. Our findings revealed a mismatch between how instructors designed for learning and how students study. In most weeks, students spent less time studying the assigned materials on the virtual learning environment (VLE) compared to the number of hours recommended by instructors. The timing of engagement also varied, from in advance to catching up patterns. High-performing students spent more time studying in advance, while low-performing students spent a higher proportion of their time on catching-up activities. By incorporating the pedagogical context into learning analytics, not only we can understand what, why, and when students engage, but also how their behaviours are influenced by the way instructors design for learning."
9,"
Topic Dependency Models: Graph-Based Visual Analytics for Communicating Assessment Data
","
                                                                                    Hassan Khosravi,                                                            Kendra Cooper                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6093,"Educational environments continue to evolve rapidly to address the needs of diverse, growing student populations while embracing advances in pedagogy and technology. In this changing landscape, ensuring consistency among the assessments for different offerings of a course (within or across terms), providing meaningful feedback about student achievements, and tracking student progress over time are all challenging tasks, particularly at scale. Here, a collection of visual Topic Dependency Models (TDMs) is proposed to help address these challenges. It uses statistical models to determine and visualize student achievements on one or more topics and their dependencies at a course level reference TDM (e.g., CS 100) as well as assessment data at the classroom level (e.g., students in CS 100 Term 1 2016 Section 001), both at one point in time (static) and over time (dynamic). The collection of TDMs share a common two-weighted graph foundation. Exemplar algorithms are presented for the creation of the course reference and selected class (static and dynamic) TDMs; the algorithms are illustrated using a common symbolic example. Studies on the application of the TDM collection on datasets from two university courses are presented; these case studies utilize the open-source, proof of concept tool under development."
10,"
Quantitative and Qualitative Analysis of the Learning Analytics and Knowledge Conference 2018
","
                                                                                    Xavier Ochoa,                                                            Agathe Merceron                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/6375,Editorial of the Special Section of LAK-18
11,"
Student Ability Best Predicts Final Grade in a College Algebra Course
","
                                                                                    Kyle Anthony O'Connell,                                                            Elijah Wostl,                                                            Matt Crosslin,                                                            T. Lisa Berry,                                                            James P. Grover                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/5833,"Historical student data can help elucidate the factors that promote student success in mathematics courses. Herein we use both multiple regression and principal component analyses to explore ten years of historical data from over 20,000 students in an introductory college-level Algebra course in an urban American research university with a diverse student population in order to understand the relationship between course success and student performance in previous courses, student demographic background, and time spent on coursework. We find that indicators of students’ past performance and experience, including grade-point-average and the number of accumulated credit hours, best predict student success in this course. We also find that overall final grades are representative of the entire course and are not unduly weighted by any one topic. Furthermore, the amount of time spent working on assignments led to improved grade outcomes. With these baseline data, our team plans to design targeted interventions that can increase rates of student success in future courses."
12,"
Impact of Lecturer’s Discourse for Students’ Video Engagement: Video Learning Analytics Case Study of MOOCs
","
                                                                                    Thushari Atapattu,                                                            Katrina Falkner                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/5748,"Atapattu, T., K. Falkner and H. Tarmazdi (2016). Topic-wise classification of MOOC discussions: A visual analytics approach. Proceedings of the 9th International conference on Educational Data Mining, Raleigh, NC, USA. "
13,"
What Constitutes an ‘Actionable Insight’ in Learning Analytics?
","
                                                                                    Rasmus Leth Jørnø,                                                            Karsten Gynther                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/5897,"The possibilities of Learning Analytics as a tool for empowering teachers and educators have created a steep interest in how to provide so-called actionable insights. However, the literature offers little in the way of defining or discussing what the term “actionable insight” means. This selective literature review provides a look into the use of the term in current literature. The review points to a dominant perspective in the literature that assumes the perspective of a rational actor, where actionable insights are treated as insights mined from data and subsequently acted upon. It also finds evidence of other perspectives and discusses the need for clarification of the term in order to establish a more precise and fruitful use of the term."
14,"
A Method for Automatically Analyzing Intelligent Tutoring System Dialogues with Coh-Metrix
","
                                                                                    Chrstopher R. Wolfe,                                                            Colin L. Widmer,                                                            Christine V. Torrese,                                                            Mitchell Dandignac                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/5901,"We developed a method for using Coh-Metrix to automatically analyze tutorial dialogues. Coh-Metrix, a web-based tool for automatically evaluating text, is freely available to researchers. We applied the method to 190 tutorial dialogues between women and BRCA Gist from two experiments. BRCA Gist is an intelligent tutoring system (ITS) to help women make decisions about genetic testing for breast cancer risk. Tutorial dialogues scored high on measures of textual cohesion (deep cohesion, referential cohesion, and the composite variable formality). They also scored high on measures of the situation model (LSA verb overlap and causal verb and causal particle). However, there was mixed support for the hypothesis that higher scores on Coh-Metrix variables would predict subsequent comprehension. A Coh-Metrix principle is that the observable cohesion of a text is a reliable guide to the coherence of the reader’s mental representation of that text. Thus it appears that interacting with BRCA Gist helped people form coherent mental representations of complex medical materials. We conclude that Coh-Metrix can be used to reliably assess tutorial dialogues and make inferences about the mental representations of people engaged in conversation with an ITS based on observable characteristics of the statements people make."
15,"
OnTask: Delivering Data-Informed, Personalized Learning Support Actions
","
                                                                                    Abelardo Pardo,                                                            Kathryn Bartimote,                                                            Simon Buckingham Shum,                                                            Shane Dawson,                                                            Jing Gao,                                                            Dragan Gašević,                                                            Steve Leichtweis,                                                            Danny Liu,                                                            Roberto Martínez-Maldonado,                                                            Negin Mirriahi,                                                            Adon Christian Michael Moskal,                                                            Jurgen Schulte,                                                            George Siemens,                                                            Lorenzo Vigentini                                                                        ",https://learning-analytics.info/journals/index.php/JLA/article/view/5988,"The learning analytics community has matured significantly over the past few years as a middle space where technology and pedagogy combine to support learning experiences. To continue to grow and connect these perspectives, research needs to move beyond the level of basic support actions. This means exploring the use of data to prove richer forms of actions, such as personalized feedback, or hybrid approaches where instructors interpret the outputs of algorithms and select an appropriate course of action. This paper proposes the following three contributions to connect data extracted from the learning experience with such personalized student support actions: 1) a student–instructor centred conceptual model connecting a representation of the student information with a basic set of rules created by instructors to deploy Personalized Learning Support Actions (PLSAs); 2) a software architecture based on this model with six categories of functional blocks to deploy the PLSAs; and 3) a description of the implementation of this architecture as an open-source platform to promote the adoption and exploration of this area."
